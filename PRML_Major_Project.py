# -*- coding: utf-8 -*-
"""PRML_Major_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mVIksKjhdq38kEunOIeXZz-wO0z88Wx9

*  Patel Samarth Rajeshkumar (B21CS057)
*  Pappusani Sai Kiran Reddy (B21CS056)
*  Ved Brahmbhatt            (B21EE075)

# Importing Dataset
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d tawsifurrahman/covid19-radiography-database

!unzip /content/covid19-radiography-database.zip

"""# Importing Necessary Libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import sys
import seaborn as sns
from tqdm import tqdm
import IPython
import numpy as np
import wave
from sklearn.cluster import KMeans
import os
from PIL import Image
from numpy import asarray
import cv2
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score,f1_score,roc_auc_score, recall_score, precision_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import *
from matplotlib import pyplot
from xgboost.sklearn import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier

import torch
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from torchvision.datasets import ImageFolder
from torchvision.datasets.folder import IMG_EXTENSIONS, default_loader
import torch.nn as nn
import torch.optim as optim

"""# Organizing the data for Deep Learning Models"""

covid_path = '/content/COVID-19_Radiography_Dataset/COVID/images'
normal_path = '/content/COVID-19_Radiography_Dataset/Normal/images'

covid_masked_path = '/content/COVID-19_Radiography_Dataset/COVID/masks'
normal_masked_path = '/content/COVID-19_Radiography_Dataset/Normal/masks'

print("Number of images for covid positive are ", len(list(os.listdir(covid_path))))
print("Number of images for Normal are ", len(list(os.listdir(normal_path))))
print("Number of masked images for covid positive are ", len(list(os.listdir(covid_masked_path))))
print("Number of masked images for covid positive are ", len(list(os.listdir(normal_masked_path))))

"""# Creating Dataset and Dataloader and visaulizing images"""

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, covid_path, normal_path, transform=None):
        """
        Constructor for the CustomDataset class.

        Args:
        - covid_path (str): Path to the directory containing COVID-19 images.
        - normal_path (str): Path to the directory containing normal images.
        - transform (callable, optional): A function/transform that takes in an image and returns a transformed version.
        """
        self.covid_path = covid_path
        self.normal_path = normal_path
        self.transform = transform

        # Get the paths of all COVID-19 and normal images in their respective directories.
        self.covid_images = [os.path.join(covid_path, f) for f in os.listdir(covid_path) if f.endswith('.png') or f.endswith('.jpg')]
        self.normal_images = [os.path.join(normal_path, f) for f in os.listdir(normal_path) if f.endswith('.png') or f.endswith('.jpg')]

    def __getitem__(self, index):
        """
        Gets the image and label at the specified index.

        Args:
        - index (int): Index of the image to retrieve.

        Returns:
        - img (PIL.Image.Image): The image at the specified index.
        - label (int): The label of the image at the specified index (0 if COVID-19, 1 if normal).
        """
        if index < len(self.covid_images):
            img_path = self.covid_images[index]
            label = 0  # COVID-19
        else:
            img_path = self.normal_images[index - len(self.covid_images)]
            label = 1  # Normal

        # Load the image.
        img = Image.open(img_path).convert('RGB')

        # Apply the transformation (if provided).
        if self.transform is not None:
            img = self.transform(img)

        return img, label

    def __len__(self):
        """
        Returns the length of the dataset.

        Returns:
        - (int): The total number of images in the dataset.
        """
        return len(self.covid_images) + len(self.normal_images)

# Define the image transformations to be applied
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resizes the image to (224, 224)
    transforms.ToTensor(),  # Converts the image to a PyTorch tensor
])

# Create a custom dataset
dataset = CustomDataset(covid_path, normal_path, transform=transform)
"""
Args:
- covid_path (str): Path to the directory containing COVID-19 images.
- normal_path (str): Path to the directory containing normal images.
- transform (callable, optional): A function/transform that takes in an image and returns a transformed version.
"""

# Split the dataset into train and test sets
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])
"""
Args:
- dataset (Dataset): The dataset to be split.
- lengths (sequence): A sequence of integers defining the lengths of the splits to be produced.
"""

# Create data loaders
batch_size = 32
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
"""
Args:
- dataset (Dataset): The dataset to load the data from.
- batch_size (int, optional): How many samples per batch to load.
- shuffle (bool, optional): Set to True to have the data reshuffled at every epoch.
- drop_last (bool, optional): Set to True to drop the last incomplete batch if the dataset size is not divisible by the batch size.
- num_workers (int, optional): How many subprocesses to use for data loading. 
  0 means that the data will be loaded in the main process. 
  Set it to higher numbers (e.g. 2) to speed up data loading if your CPU and memory have more cores to use.
"""

# Select some random images from the dataset
num_images = 10
random_indices = np.random.choice(len(dataset), size=num_images, replace=False)
random_images = [dataset[i][0] for i in random_indices]
random_labels = [dataset[i][1] for i in random_indices]

# Create a grid of images
fig, axes = plt.subplots(nrows=2, ncols=num_images//2, figsize=(15, 5))

# Display the images
for i in range(num_images):
    ax = axes[i//5, i%5]  # Compute the row and column index of the current subplot
    ax.imshow(random_images[i].permute(1, 2, 0))  # Display the image with channel ordering (C, H, W) -> (H, W, C)
    ax.axis('off')  # Turn off the axis
    ax.set_title('COVID' if random_labels[i] == 0 else 'Normal')  # Set the title based on the label
    
plt.show()

"""# Applying CNN model from scratch"""

from google.colab import drive
drive.mount('/content/drive')

import torch.nn as nn

# Define a CNN model by subclassing nn.Module
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()  # Call the constructor of nn.Module
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # Define a convolutional layer with 3 input channels, 16 output channels, kernel size 3x3, and padding 1
        self.pool = nn.MaxPool2d(2, 2)  # Define a max pooling layer with kernel size 2x2 and stride 2
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # Define another convolutional layer with 16 input channels, 32 output channels, kernel size 3x3, and padding 1
        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Define a fully connected layer with 32 x 56 x 56 input features and 128 output features
        self.fc2 = nn.Linear(128, 2)  # Define the output layer with 128 input features and 2 output features (for COVID and normal classes)

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))  # Apply the first convolutional layer, ReLU activation function, and max pooling
        x = self.pool(nn.functional.relu(self.conv2(x)))  # Apply the second convolutional layer, ReLU activation function, and max pooling
        x = torch.flatten(x, 1)  # Flatten the output tensor to a 1D tensor
        x = nn.functional.relu(self.fc1(x))  # Apply a ReLU activation function to the output of the fully connected layer
        x = self.fc2(x)  # Apply the output layer
        return x  # Return the output tensor

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Initialize the model and move it to the GPU
model = Net()  # create a new instance of the Net class
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # determine the device (GPU or CPU) to use
model = nn.DataParallel(model)  # allow the model to utilize multiple GPUs
model.to(device)  # move the model to the device

# Define the loss function and optimizer
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()  # define the loss function
optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # define the optimizer

# Define variables to keep track of the best validation accuracy and the corresponding model state
best_acc = 0.0  # initialize the best validation accuracy to 0.0
best_state = None  # initialize the best model state to None

# Train the model
for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()  # set the gradients to zero
        outputs = model(inputs)  # forward pass
        loss = criterion(outputs, labels)  # compute the loss
        loss.backward()  # backward pass
        optimizer.step()  # update the weights

        running_loss += loss.item()
        if i % 100 == 99:
            # print the average loss for the last 100 batches
            print(f'Epoch {epoch+1}, batch {i+1}: loss = {running_loss/100:.4f}')
            running_loss = 0.0

    # Validate the model
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Update the best model state if the validation accuracy improves
    acc = 100 * correct / total
    if acc > best_acc:
        best_acc = acc
        best_state = model.state_dict()
        torch.save(best_state, 'best_model.pth')  # save the best model state to disk

    # print the validation accuracy for the current epoch
    print(f'Epoch {epoch+1}: validation accuracy = {100 * correct / total:.2f}%')

# Load the best model state
model.load_state_dict(best_state)  # load the best model state from disk

# Load the best model
model_best = Net()   # initialize a new model
model_best = nn.DataParallel(model_best)  # parallelize the model across multiple GPUs
model_best.load_state_dict(torch.load('/content/drive/MyDrive/models/best_model.pth'))  # load the state of the best model
model_best.to(device)  # move the model to the GPU

# Test the best model
total = 0
correct = 0
y_true_cnn = []  # initialize a list to store the true labels
y_pred_cnn = []  # initialize a list to store the predicted labels
with torch.no_grad():
    for data in test_loader:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_best(inputs)
        _, predicted = torch.max(outputs.data, 1)  # get the index of the max log-probability
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        y_true_cnn += labels.cpu().tolist()
        y_pred_cnn += predicted.cpu().tolist()

acc_cnn = 100 * correct / total  # calculate the test accuracy
print(f'Test accuracy of best model: {acc_cnn:.2f}%')
f1_cnn = f1_score(y_true_cnn, y_pred_cnn, average='macro')  # calculate the macro F1 score
print(f'F1 score of best model: {f1_cnn:.2f}')

"""# Applying ResNet18 Model"""

model_resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)
model_resnet18.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Define transformation pipeline for ResNet model input
transform_resnet = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create a custom dataset for the ResNet model
dataset_resnet = CustomDataset(covid_path, normal_path, transform=transform_resnet)

# Split the dataset into train and test sets with 80:20 ratio
train_size = int(0.8 * len(dataset_resnet))
test_size = len(dataset_resnet) - train_size
train_dataset_resnet, test_dataset_resnet = torch.utils.data.random_split(dataset_resnet, [train_size, test_size])

# Create data loaders for the train and test datasets
batch_size = 32
train_loader_resnet = torch.utils.data.DataLoader(train_dataset_resnet, batch_size=batch_size, shuffle=True)
test_loader_resnet = torch.utils.data.DataLoader(test_dataset_resnet, batch_size=batch_size, shuffle=False)

# Move the model to the specified device
model_resnet18.to(device)

# Define the loss function and optimizer
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_resnet18.parameters(), lr=learning_rate)

# Define variables to keep track of the best validation accuracy and the corresponding model state
best_acc = 0.0
best_state = None

# Train the model for 20 epochs
for epoch in range(20):
    running_loss = 0.0
    # Iterate over the training data in batches
    for i, data in enumerate(train_loader_resnet, 0):
        inputs, labels = data
        # Move the inputs and labels to the specified device
        inputs, labels = inputs.to(device), labels.to(device)
        # Zero the gradients
        optimizer.zero_grad()
        # Forward pass
        outputs = model_resnet18(inputs)
        # Compute the loss
        loss = criterion(outputs, labels)
        # Backward pass
        loss.backward()
        # Update the parameters
        optimizer.step()

        # Print statistics every 100 batches
        running_loss += loss.item()
        if i % 100 == 99:
            print(f'Epoch {epoch+1}, batch {i+1}: loss = {running_loss/100:.4f}')
            running_loss = 0.0

    # Validate the model on the test set
    correct = 0
    total = 0
    with torch.no_grad():
        # Iterate over the test data in batches
        for data in test_loader_resnet:
            inputs, labels = data
            # Move the inputs and labels to the specified device
            inputs, labels = inputs.to(device), labels.to(device)
            # Forward pass
            outputs = model_resnet18(inputs)
            # Compute the predicted labels
            _, predicted = torch.max(outputs.data, 1)
            # Update the total and correct counts
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Update the best model state if the validation accuracy improves
    acc = 100 * correct / total
    if acc > best_acc:
        best_acc = acc
        best_state = model_resnet18.state_dict()
        # Save the best model state to a file
        torch.save(best_state, 'best_model_resnet18.pth')

    # Print the validation accuracy for the epoch
    print(f'Epoch {epoch+1}: validation accuracy = {100 * correct / total:.2f}%')

# Load the best model state
model_resnet18.load_state_dict(best_state)

model_resnet18.load_state_dict(torch.load('/content/drive/MyDrive/models/best_model_resnet18.pth'))
model_resnet18.to(device)

# Test the best model

# Initialize variables for keeping track of accuracy and predictions
total = 0
correct = 0
y_true_resnet18 = [] # True labels
y_pred_resnet18 = [] # Predicted labels

# Evaluate the best model on the test set
with torch.no_grad():
    for data in test_loader_resnet:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_resnet18(inputs)
        _, predicted = torch.max(outputs.data, 1) # Get the index of the class with the highest probability
        total += labels.size(0)
        correct += (predicted == labels).sum().item() # Count the number of correct predictions
        y_true_resnet18 += labels.cpu().tolist() # Append the true labels to the y_true list
        y_pred_resnet18 += predicted.cpu().tolist() # Append the predicted labels to the y_pred list

# Compute the test accuracy and F1 score
acc_resnet18 = 100 * correct / total
print(f'Test accuracy of best resnet18 model: {acc_resnet18:.2f}%')
f1_resnet18 = f1_score(y_true_resnet18, y_pred_resnet18, average='macro')
print(f'F1 score of best renset18 model: {f1_resnet18:.2f}')

"""# Applying InceptionV3 Model"""

model_inceptionv3 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)
model_inceptionv3.eval()

transform_inceptionv3 = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create a custom dataset
dataset_inceptionv3 = CustomDataset(covid_path, normal_path, transform=transform_inceptionv3)

# Split the dataset into train and test sets
train_size = int(0.8 * len(dataset_inceptionv3))
test_size = len(dataset_inceptionv3) - train_size
train_dataset_inceptionv3, test_dataset_inceptionv3 = torch.utils.data.random_split(dataset_inceptionv3, [train_size, test_size])

# Create data loaders
batch_size = 32
train_loader_inceptionv3 = torch.utils.data.DataLoader(train_dataset_inceptionv3, batch_size=batch_size, shuffle=True)
test_loader_inceptionv3 = torch.utils.data.DataLoader(test_dataset_inceptionv3, batch_size=batch_size, shuffle=False)

model_inceptionv3.to(device)

# Define the loss function and optimizer
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_inceptionv3.parameters(), lr=learning_rate)

# Define variables to keep track of the best validation accuracy and the corresponding model state
best_acc = 0.0
best_state = None

# Train the model
for epoch in range(20):
    running_loss = 0.0
    for i, data in enumerate(train_loader_inceptionv3, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model_inceptionv3(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f'Epoch {epoch+1}, batch {i+1}: loss = {running_loss/100:.4f}')
            running_loss = 0.0

    # Validate the model
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader_inceptionv3:
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model_inceptionv3(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Update the best model state if the validation accuracy improves
    acc = 100 * correct / total
    if acc > best_acc:
        best_acc = acc
        best_state = model_inceptionv3.state_dict()
        torch.save(best_state, 'best_model_inceptionv3.pth')

    print(f'Epoch {epoch+1}: validation accuracy = {100 * correct / total:.2f}%')

# Load the best model state
model_inceptionv3.load_state_dict(best_state)

model_inceptionv3.load_state_dict(torch.load('/content/drive/MyDrive/models/best_model_inceptionv3.pth'))
model_inceptionv3.to(device)

# Test the best model
total = 0
correct = 0
y_true_inceptionv3 = []
y_pred_inceptionv3 = []
with torch.no_grad():
    for data in test_loader_inceptionv3:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_inceptionv3(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        y_true_inceptionv3 += labels.cpu().tolist()
        y_pred_inceptionv3 += predicted.cpu().tolist()

acc_inceptionv3 = 100 * correct / total
print(f'Test accuracy of best inceptionv3 model: {acc_inceptionv3:.2f}%')
f1_inceptionv3 = f1_score(y_true_inceptionv3, y_pred_inceptionv3, average='macro')
print(f'F1 score of best inceptionv3 model: {f1_inceptionv3:.2f}')

"""# Applying VGG11 Model"""

model_vgg11 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)
model_vgg11.eval()

transform_vgg11 = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create a custom dataset
dataset_vgg11 = CustomDataset(covid_path, normal_path, transform=transform_vgg11)

# Split the dataset into train and test sets
train_size = int(0.8 * len(dataset_vgg11))
test_size = len(dataset_vgg11) - train_size
train_dataset_vgg11, test_dataset_vgg11 = torch.utils.data.random_split(dataset_vgg11, [train_size, test_size])

# Create data loaders
batch_size = 32
train_loader_vgg11 = torch.utils.data.DataLoader(train_dataset_vgg11, batch_size=batch_size, shuffle=True)
test_loader_vgg11 = torch.utils.data.DataLoader(test_dataset_vgg11, batch_size=batch_size, shuffle=False)

model_vgg11.to(device)

# Define the loss function and optimizer
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_vgg11.parameters(), lr=learning_rate)

# Define variables to keep track of the best validation accuracy and the corresponding model state
best_acc = 0.0
best_state = None

# Train the model
for epoch in range(20):
    running_loss = 0.0
    for i, data in enumerate(train_loader_vgg11, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model_vgg11(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f'Epoch {epoch+1}, batch {i+1}: loss = {running_loss/100:.4f}')
            running_loss = 0.0

    # Validate the model
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader_vgg11:
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model_vgg11(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Update the best model state if the validation accuracy improves
    acc = 100 * correct / total
    if acc > best_acc:
        best_acc = acc
        best_state = model_vgg11.state_dict()
        torch.save(best_state, 'best_model_vgg_11.pth')

    print(f'Epoch {epoch+1}: validation accuracy = {100 * correct / total:.2f}%')

# Load the best model state
model_vgg11.load_state_dict(best_state)

model_vgg11.load_state_dict(torch.load('/content/drive/MyDrive/models/best_model_vgg_11.pth'))
model_vgg11.to(device)

# Test the best model
total = 0
correct = 0
y_true_vgg11 = []
y_pred_vgg11 = []
with torch.no_grad():
    for data in test_loader_vgg11:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_vgg11(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        y_true_vgg11 += labels.cpu().tolist()
        y_pred_vgg11 += predicted.cpu().tolist()

acc_vgg11 = 100 * correct / total
print(f'Test accuracy of best vgg11 model: {acc_vgg11:.2f}%')
f1_vgg11 = f1_score(y_true_vgg11, y_pred_vgg11, average='macro')
print(f'F1 score of best vgg11 model: {f1_vgg11:.2f}')

"""# Applying AlexNet Model"""

model_alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)
model_alexnet.eval()

transform_alexnet = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create a custom dataset
dataset_alexnet = CustomDataset(covid_path, normal_path, transform=transform_alexnet)

# Split the dataset into train and test sets
train_size = int(0.8 * len(dataset_alexnet))
test_size = len(dataset_alexnet) - train_size
train_dataset_alexnet, test_dataset_alexnet = torch.utils.data.random_split(dataset_alexnet, [train_size, test_size])

# Create data loaders
batch_size = 32
train_loader_alexnet = torch.utils.data.DataLoader(train_dataset_alexnet, batch_size=batch_size, shuffle=True)
test_loader_alexnet = torch.utils.data.DataLoader(test_dataset_alexnet, batch_size=batch_size, shuffle=False)

model_alexnet.to(device)

# Define the loss function and optimizer
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_alexnet.parameters(), lr=learning_rate)

# Define variables to keep track of the best validation accuracy and the corresponding model state
best_acc = 0.0
best_state = None

# Train the model
for epoch in range(20):
    running_loss = 0.0
    for i, data in enumerate(train_loader_alexnet, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model_alexnet(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:
            print(f'Epoch {epoch+1}, batch {i+1}: loss = {running_loss/100:.4f}')
            running_loss = 0.0

    # Validate the model
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader_alexnet:
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model_alexnet(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    # Update the best model state if the validation accuracy improves
    acc = 100 * correct / total
    if acc > best_acc:
        best_acc = acc
        best_state = model_alexnet.state_dict()
        torch.save(best_state, 'best_model_alexnet.pth')

    print(f'Epoch {epoch+1}: validation accuracy = {100 * correct / total:.2f}%')

# Load the best model state
model_alexnet.load_state_dict(best_state)

model_alexnet.load_state_dict(torch.load('/content/drive/MyDrive/models/best_model_alexnet.pth'))
model_alexnet.to(device)

# Test the best model
total = 0
correct = 0
y_true_alexnet = []
y_pred_alexnet = []
with torch.no_grad():
    for data in test_loader_alexnet:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_alexnet(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        y_true_alexnet += labels.cpu().tolist()
        y_pred_alexnet += predicted.cpu().tolist()

acc_alexnet = 100 * correct / total
print(f'Test accuracy of best alexnet model: {acc_alexnet:.2f}%')
f1_alexnet = f1_score(y_true_alexnet, y_pred_alexnet, average='macro')
print(f'F1 score of best alexnet model: {f1_alexnet:.2f}')

"""# Comparing all deep learning models"""

y_pred_model = {'resnet18':y_pred_resnet18, 'vgg11':y_pred_vgg11, 'alexnet':y_pred_alexnet, 'inceptionv3':y_pred_inceptionv3, 'cnn':y_pred_cnn}
y_true_model = {'resnet18':y_true_resnet18, 'vgg11':y_true_vgg11, 'alexnet':y_true_alexnet, 'inceptionv3':y_true_inceptionv3, 'cnn':y_true_cnn}

models = ['resnet18', 'vgg11', 'alexnet', 'inceptionv3', 'cnn']
results = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'])

for model in models:
    y_pred = y_pred_model[model]
    y_true = y_true_model[model]
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    row = pd.DataFrame({'Model': model, 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'ROC AUC': roc_auc}, index=[0])
    results = pd.concat([results, row], ignore_index=True)

results

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the ROC curve
for i, name in enumerate(models):
    fpr, tpr, thresholds = roc_curve(y_true_model[name], y_pred_model[name])
    ax = axes[i//3][i%3]
    ax.plot(fpr, tpr)
    ax.plot([0, 1], [0, 1], linestyle='--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.0])
    ax.set_xlabel('False Positive Rate')
    ax.set_ylabel('True Positive Rate')
    ax.set_title(f"ROC curve for {name}")

plt.tight_layout()
plt.show()

class_names = ['Covid', 'Normal']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, name in enumerate(models):
    cm = confusion_matrix(y_true_model[name], y_pred_model[name])
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()

# Create a list of accuracy scores for each model
accuracy_scores = [accuracy_score(y_true_model[model], y_pred_model[model]) for model in models]

# Create a line plot
plt.plot(models, accuracy_scores, '-o')

# Set the axis labels and title
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy Scores for Different Models')

# Set the axis ticks and limits
plt.xticks(rotation=45)

# Display the plot
plt.show()

# Create a list of accuracy scores for each model
F1_Scores = [f1_score(y_true_model[model], y_pred_model[model]) for model in models]

# Create a line plot
plt.plot(models, F1_Scores, '-o', color='y')

# Set the axis labels and title
plt.xlabel('Model')
plt.ylabel('F1_Scores')
plt.title('F1 Scores for Different Models')

# Set the axis ticks and limits
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""# Applying PCA to one image to reduce dimensions and check the transformed image"""

dic={}

c=0
lst=list(os.listdir(covid_path))
for i in tqdm(range(len(list(os.listdir(covid_path))))):
  dic[c]=covid_path + '/' + lst[i]
  c+=1

dic_normal={}

c=0
lst=list(os.listdir(normal_path))
for i in tqdm(range(len(list(os.listdir(normal_path))))):
  dic_normal[c]=normal_path+'/'+lst[i]
  c+=1

# Loading the image 
img = cv2.imread(dic_normal[8]) #you can use any image you want.
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img = cv2.resize(img, dsize=(224,224), interpolation=cv2.INTER_CUBIC)
plt.imshow(img,cmap='gray')

img.shape

pca = PCA(n_components=20)
pca.fit(img)
trans_img = pca.transform(img)

trans_img.shape

trans_img.flatten().shape

var=(pca.explained_variance_ratio_)
cum_var_exp = np.cumsum(var)
cum_var_exp = np.cumsum(var)

# plot explained variances
plt.bar(range(1,len(var)+1), var, alpha=0.5,
        align='center', label='individual explained variance')
plt.step(range(1,len(var)+1), cum_var_exp, where='mid',
         label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

print(cum_var_exp)

"""Taking 20 PCs to cover 99% variance"""

rows = 1
columns = 2
img_transformed = pca.inverse_transform(trans_img)

fig = plt.figure(figsize=(10, 7))
fig.add_subplot(rows, columns, 1)
plt.imshow(img,cmap='gray')
plt.axis('off')
plt.title("Original Image")

fig.add_subplot(rows, columns, 2)
plt.imshow(img_transformed,cmap='gray')
plt.axis('off')
plt.title("Blurred Image after applying PCA")

"""# Applying PCA over all the images with n_components=20"""

dic_keys=list(dic.keys())
array_covid=np.zeros((1,4481))
for i in tqdm(range(len(dic_keys))):
  img = cv2.imread(dic[i]) #you can use any image you want.
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  img = cv2.resize(img, dsize=(224,224), interpolation=cv2.INTER_CUBIC)
  pca = PCA(n_components=20)
  pca.fit(img)
  trans_img = pca.transform(img)
  array_covid=np.append(array_covid,np.concatenate((trans_img.flatten(),np.array([1])),axis=0).reshape(1,4481),axis=0)

array_covid.shape

dic_normal_keys=list(dic_normal.keys())
array_normal=np.zeros((1,4481))
for i in tqdm(range(len(dic_normal_keys))):
  img = cv2.imread(dic_normal[i]) #you can use any image you want.
  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  img = cv2.resize(img, dsize=(224,224), interpolation=cv2.INTER_CUBIC)
  pca = PCA(n_components=20)
  pca.fit(img)
  trans_img = pca.transform(img)
  array_normal=np.append(array_normal,np.concatenate((trans_img.flatten(),np.array([0])),axis=0).reshape(1,4481),axis=0)

array_normal.shape

"""# Creating Dataset of PCA transformed images"""

data_array=np.concatenate((array_covid,array_normal),axis=0)
data_array.shape

X=data_array.T[:-1].T
y=data_array.T[-1].T

X[0].shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

y_train

"""# Applying various supervised machine learning models to dataset"""

# Define the models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(probability=True),
    'XgBoost': XGBClassifier(),
    'LightGBM': LGBMClassifier()
    
}

best_models = {}
for name, model in models.items():
    clf = model
    clf.fit(X_train, y_train)
    best_models[name] = clf
    print(f"Model for {name} built.")

# Evaluate the best models on the testing set
results = {}
for name, model in best_models.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    results[name] = {
        'Accuracy': accuracy,
        'F1 score': f1,
        'Precision': precision,
        'Recall': recall,
        'ROC AUC': roc_auc,
        'FPR': fpr,
        'TPR': tpr,
        'Thresholds': thresholds
    }

"""# Comparing all supervised machine learning models"""

# Create a DataFrame to store the results
df_results = pd.DataFrame(columns=['Model', 'Accuracy', 'F1 score', 'Precision', 'Recall', 'ROC AUC'])

for name, metrics in results.items():
  accuracy = metrics['Accuracy']
  f1 = metrics['F1 score']
  precision = metrics['Precision']
  recall = metrics['Recall']
  roc_auc = metrics['ROC AUC']
  df_results = df_results.append({
    'Model': name,
    'Accuracy': accuracy,
    'F1 score': f1,
    'Precision': precision,
    'Recall': recall,
    'ROC AUC': roc_auc
  }, ignore_index=True)

df_results

accs = [0.707096, 0.758508, 0.812817, 0.803041, 0.830196, 0.842143, 0.858436]

model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN','SVM','XgBoost','LightGBM']

# Create a line plot
plt.plot(model_names, accs, '-o')

# Set the axis labels and title
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy Scores for Different Models')

# Set the axis ticks and limits
plt.xticks(rotation=45)

# Display the plot
plt.show()

f1_score = [0.381025, 0.547797, 0.526990, 0.609195, 0.615889, 0.657771, 0.694292]

model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN','SVM','XgBoost','LightGBM']

# Create a line plot
plt.plot(model_names, f1_score, '-o', color='y')

# Set the axis labels and title
plt.xlabel('Model')
plt.ylabel('F1 Scores')
plt.title('F1 Scores for Different Models')

# Set the axis ticks and limits
plt.xticks(rotation=45)

# Display the plot
plt.show()

# Create a grid of subplots for ROC curves
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15,10))

# Plot ROC curves for each model on a separate subplot
for ax, (name, metrics) in zip(axes.flat, results.items()):
  fpr = metrics['FPR']
  tpr = metrics['TPR']
  roc_auc = metrics['ROC AUC']
  ax.plot([0, 1], [0, 1], linestyle='--')
  ax.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")
  ax.set_xlabel('False Positive Rate')
  ax.set_ylabel('True Positive Rate')
  ax.set_title(f'ROC Curve for {name}')
  ax.legend()
# Adjust spacing between subplots
plt.tight_layout()
# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix

class_names = ['Covid', 'Normal']

# Define the models and their names
models_ = [best_models['Logistic Regression'],
          best_models['Decision Tree'],
          best_models['Random Forest'],
          best_models['KNN'],
          best_models['SVM'],
          best_models['XgBoost'],
          best_models['LightGBM']]
model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'KNN','SVM','XgBoost','LightGBM']

# Create a grid of subplots
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))

# Loop through each model and its corresponding name and plot the confusion matrix
for i, (name, model) in enumerate(zip(model_names, models_)):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    ax = axes[i//3][i%3]
    sns.heatmap(cm, annot=True, fmt='g', cmap=plt.cm.Blues, ax=ax, xticklabels=class_names, yticklabels=class_names)
    ax.set_title(f"Confusion matrix for {name}")
    ax.set_xlabel('Predicted label')
    ax.set_ylabel('True label')

plt.tight_layout()
plt.show()



"""# Deep Learning models were trained here

https://colab.research.google.com/drive/1q7NaRgD59VPFlT-Qe07Vy-yZ-KFqiqft?usp=sharing

https://colab.research.google.com/drive/1W9X_CYbbLW-g-MNYWx1OIH79m2k2QkS8?usp=sharing

Drive link for all the best models downloaded

https://drive.google.com/drive/folders/1GAfkh404fPgB2MAA9u00U4wISszhUoyp?usp=sharing
"""